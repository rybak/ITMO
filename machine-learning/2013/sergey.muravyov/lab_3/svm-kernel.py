import numpy as npfrom math import expfrom cvxopt import matrixfrom cvxopt import solversfrom urllib.request import urlopenfrom random import randintdef shuffle(x, y):    tmp = list(zip(x, y))    np.random.shuffle(tmp)    x_new = []    y_new = []    for i in range(len(tmp)):        x_new.append(tmp[i][0])        y_new.append(tmp[i][1])    return x_new, y_newdef get_data():    x, y = [], []    file = urlopen("http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data")    for line in file.readlines():        input = line.decode('utf-8').strip().split(',')        if input[1] == 'M':            y.append(1.0)        else:            y.append(-1.0)        num = [float(x) for x in input[2:]]        num.insert(0, 1.0)        num = np.array(num)        x.append(num)    file.close()    return shuffle(x, y)def split_xy(x, y, i, p):    return x[i * p: (i + 1) * p], x[:i * p] + x[(i + 1) * p:], y[i * p: (i + 1) * p], y[:i * p] + y[(i + 1) * p:]def gaussian_K(x, y):    return exp(-np.sum((x - y) ** 2) * 1e-6 / 2)def polynomial_K(x, y):    return float(1 + np.dot(x, y)) ** 3def scalar(x, y):    return float(np.dot(x, y))def count_b(Ai, Aj, b1, b2, c):    b = 0    if 0 < Ai < c:        b = b1    elif 0 < Aj < c:        b = b2    else:        b = (b1 + b2) / 2    return b    def smo_simple(x, y, c, kernel, eps=1e-9):    n, d, b = len(x), len(x[0])-1, 0    A, K = np.zeros(n), np.empty((n, n))        for i in range(n):        for j in range(n):            K[i][j] = kernel(x[i], x[j])                func = lambda i: b + np.sum(np.dot(K[i], A * y))    def minmax(p, lo, hi):        return lo if p < lo else (hi if p > hi else p)        flag = True    while flag:        flag = False        for i in range(n):            Ei = func(i) - y[i]            if (y[i] * Ei < -eps and A[i] < c) or (y[i] * Ei > eps and A[i] > 0):                j = randint(0, n - 2)                if j == i:                    j += 1                                    Ej = func(j) - y[j]                Ai_old, Aj_old = A[i], A[j]                                if y[i] == y[j]:                    L, H = max(0, Ai_old + Aj_old - c), min(c, Ai_old + Aj_old)                else:                    L, H = max(0, Aj_old - Ai_old), min(c, c + Aj_old - Ai_old)                if L >= H:                    continue                et = 2 * K[i][j] - K[i][i] - K[j][j]                if et > 0:                    continue                A[j] = minmax(A[j] - y[j] * (Ei - Ej) / et, L, H)                                if abs(A[j] - Aj_old) < 1e-5:                    continue                A[i] = A[i] + y[i] * y[j] * (Aj_old - A[j])                                b1 = b - Ei - y[i] * (A[i] - Ai_old) * kernel(x[i], x[i]) - y[j] * (A[j] - Aj_old) * kernel(x[i], x[j])                b2 = b - Ej - y[i] * (A[i] - Ai_old) * kernel(x[i], x[j]) - y[j] * (A[j] - Aj_old) * kernel(x[j], x[j])                b = count_b(A[i], A[j], b1, b2, c)                flag = True    return A, bdef get_stat(train_x, train_y, test_x, test_y, A, b1, kernel=scalar):    n, nt = len(train_y), len(test_y)    alpha, b = A, b1    func = lambda v: b + np.sum(alpha * train_y * np.apply_along_axis(lambda w: kernel(v, w), 1, train_x))        tp, fp, fn, tn = 0, 0, 0, 0    for i in range(nt):        yc = func(test_x[i])        if yc > 0 and test_y[i] == 1:            tp += 1        elif yc > 0 and test_y[i] == -1:            fp += 1        elif yc < 0 and test_y[i] == 1:            fn += 1        else:            tn += 1    precision = tp / (tp + fp) if (tp + fp) != 0 else 0    recall = tp / (tp + fn) if (tp + fn) != 0 else 0    F1 = 2 * precision * recall / (precision + recall) if (precision + recall) != 0 else 0    err = (fp + fn) / (tp + tn + fp + fn)            return precision, recall, F1, errdef predict(x, w, b):    return [-1.0 if numpy.inner(w, x[i]) + b <= 0 else 1.0 for i in range(len(x))]def classification(x, y, pr):    mc = 0    for j in range(len(x)):        if y[j] != pr[j]:            mc += 1    return mc / len(x)def get_c(x, y, parts, kernel=polynomial_K):    part_size = int(len(x) / parts)    best_f1, best_C = 0, 0    for d in range(-5, 11):        C = 2.0 ** d        test_x, train_x, test_y, train_y = split_xy(x, y, parts-1, part_size)                  A, b = smo_simple(train_x, train_y, C, kernel)        prec, rec, f1, err = get_stat(train_x, train_y, test_x, test_y, A, b, kernel)        print("C=%f"% C)           if  best_f1 < f1 or not best_f1:            best_f1 = f1            best_C = C    return best_Cdef main():    x, y = get_data()    bound = int(len(x) * 0.2)    train_x, train_y, test_x, test_y = x[bound:], y[bound:], x[:bound], y[:bound]            C = get_c(train_x, train_y, 10)    alpha, b = smo_simple(train_x, train_y, C, polynomial_K)    prec, rec, f1, err = get_stat(train_x, train_y, test_x, test_y, alpha, b, polynomial_K)    print("\nPolynomial:\n")    print("Constant = %6.5f" % C)    print("precision: %.2f\nrecall: %.2f\nF1: %.2f\nerror: %.2f\n" %(prec, rec, f1, err))    alpha, b = smo_simple(train_x, train_y, C, gaussian_K)    prec, rec, f1, err = get_stat(train_x, train_y, test_x, test_y, alpha, b, gaussian_K)    print("Gaussian:\n")    print("Constant = %6.5f" % C)    print("precision: %.2f\nrecall: %.2f\nF1: %.2f\nerror: %.2f\n" %(prec, rec, f1, err))       if __name__ == "__main__":    main()